# 大数据启蒙

## 分治思想

一个现实的具体应用：如何在`O(4)`时间内完成对1万个数据的查找？

> 不准确地说：我们可以使用一堆长度为4的链表，并把这些链表的头指针地址存在一个数组里面，如下图，这样对于每个链表的搜索时间都是`O(4)`了。还需要一种`O(1)`的方法来确定应该遍历哪个链表，此处我们可以应用hashCode的方式，计算X.hashCode%2500，得到的结果区间为`[0,2499]`，就可以当做数组的下标来找到对应的链表了。【这种实现方式类似HashTable】

<img src="https://user-images.githubusercontent.com/17522733/94971001-30b45780-0506-11eb-9420-4668dc3b5275.png" alt="以O(4)复杂度遍历10000个元素" style="zoom: 67%;" />

### 分治思想的应用举例：

- Redis集群
- ElasticSearch
- Hbase
- Hadoop生态无处不在

### 单机处理大数据问题1：

> 需求：有一个非常大 (如1TB) 的文本文件，里面有非常多行，只有两行一样，他们出现在未知的位置，需要找到他们。
>
> 限制：单机，而且内存有限。

如果使用暴力搜索的话，每次内存中保留一行数据，然后遍历整个文件进行对比，则需要`O(N^2)`的时间，对于1TB的大文件来说，一次`O(N)`就接近30分钟，这是不可接受的。

我们先进行一次遍历，并把文件中的每一行都计算器hashCode值，并进行取模运算，将得到的结果存放在内存中，这样得到的数据大小是内存可以接受的。那么每组hashCode都代表了可能相等的某些行，我们只需要针对各个组内的这些行分别进行比较即可，这次的比较总共加起来耗时将小于等于`O(N)`。

这里也是运用了分治的思想，把一个大文件分成多干个小的标签。可行是因为hashCode的计算和取模的计算都是稳定的。

<img src="https://user-images.githubusercontent.com/17522733/94975107-de2c6880-0510-11eb-8bb9-5adf5ab582f1.png" alt="快速找到大文件中的重复行" style="zoom:67%;" />

### 单机处理大数据问题2：

> 需求：有一个非常大 (1TB) 的文件，里面是无序的整型数，我们需要对其进行排序。
>
> 限制：单机，且内存有限

- 方法1：逐个数进行读取，按照该数值大小分别存放到不同编号的小文件里面（如对于x>0&&x<=1000)的值放到第1个文件里面，以此类推），一次IO即可读取所有的数据。**此时各个文件之间是外部有序，内部无序**。然后对各个文件分别进行排序，然后把这些文件合并到一起即可。总共耗时2次IO。
- 方法2：每次读取一定量的数据（如50MB），在内存中进行排序后存到一个单独的文件里。那么一次IO后就可以读取所有的数据并得到若干个小文件。**此时各个文件之间是内部有序，外部无序**。应该马上想到使用归并排序的方法。



- [x] 由于受到单机IO的瓶颈限制，需要使用多机的运算，才能在数量级上进行处理时间的加速。

### 多机处理大数据问题1：

![快速查找文件中的重复行_多机](https://user-images.githubusercontent.com/17522733/94977212-11262a80-0518-11eb-804a-d14733822a94.png)